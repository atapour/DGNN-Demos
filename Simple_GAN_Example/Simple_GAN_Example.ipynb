{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DrpK6lIWrhi"
      },
      "source": [
        "# A Simple GAN Example in PyTorch\n",
        "---\n",
        "\n",
        "## Author : Amir Atapour-Abarghouei, amir.atapour-abarghouei@durham.ac.uk\n",
        "\n",
        "This notebook will provide an example that shows the implementation of a simple Generative Adversarial Network (GAN).\n",
        "\n",
        "Copyright (c) 2024 Amir Atapour-Abarghouei, UK.\n",
        "\n",
        "License : GNU General Public License Version 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-exrFDujXlUG"
      },
      "source": [
        "We are going to implement a very simple GAN. Let's start by importing what we need:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7Xgr3ZNXmQx"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(f\"Device is {device}!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZr-PApBcN4Y"
      },
      "source": [
        "We should now line up the dataset we are going to use. We will be working with our own dataset:\n",
        "\n",
        "**[Radiant X-Ray: A Small Dataset of Processed Chest X-Ray Images](https://github.com/atapour/DGNN-Demos/tree/main/R-X-Ray)**\n",
        "\n",
        "First, let's download the dataset, which is publicly available on GitHub:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-bGhK9wmXsA_"
      },
      "outputs": [],
      "source": [
        "!wget -q -O R-X-Ray.zip https://github.com/atapour/DGNN-Demos/blob/main/R-X-Ray/R-X-Ray.zip?raw=true\n",
        "!unzip -q R-X-Ray.zip\n",
        "!rm R-X-Ray.zip\n",
        "print('done!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6AZQ9Kq_1hj"
      },
      "source": [
        "Now we are ready to process our data. We are going to convert our data to 32x32 images to make the work easier and more efficient just for demonstration purposes.\n",
        "\n",
        "Since our dataset does not have classes and is only there to train generative models, we are also going to add a fake class directory to make the implementation easier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wbTxY01eY0Aq"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import torchvision\n",
        "import torch\n",
        "\n",
        "# path to the root directory containing images\n",
        "root_dir = 'R-X-Ray'\n",
        "\n",
        "# check if there are already directories in root_dir\n",
        "subdirectories = [d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))]\n",
        "\n",
        "# if no class directories are found, create one and move the files\n",
        "if not subdirectories:\n",
        "    class_dir = os.path.join(root_dir, 'fakeclass')\n",
        "    os.makedirs(class_dir, exist_ok=True)\n",
        "\n",
        "    # move all images from the root directory to the new subdirectory\n",
        "    for file_name in os.listdir(root_dir):\n",
        "        file_path = os.path.join(root_dir, file_name)\n",
        "        if os.path.isfile(file_path):\n",
        "            shutil.move(file_path, os.path.join(class_dir, file_name))\n",
        "\n",
        "    print(f\"Created directory {class_dir} and moved images into it.\")\n",
        "else:\n",
        "    print(f\"Subdirectories already found: {subdirectories}\")\n",
        "\n",
        "# define the transformations\n",
        "transform = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.Resize([32, 32]),\n",
        "    torchvision.transforms.RandomHorizontalFlip(),\n",
        "    torchvision.transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# create the dataset and dataloader using ImageFolder\n",
        "dataset = torchvision.datasets.ImageFolder(root_dir, transform=transform)\n",
        "\n",
        "# load the dataset\n",
        "train_loader = torch.utils.data.DataLoader(dataset, batch_size=8, shuffle=True, num_workers=2)\n",
        "\n",
        "print(f\"There are {len(dataset)} images in the training set!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5UnQLwJZW6O"
      },
      "source": [
        "Our dataset is tiny and is not really suited for any real applications, but it will demonstrate the process. Let's look at a few of our images:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVqAtMBuw8LW"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "for i in range(25):\n",
        "    plt.subplot(5, 5, i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(train_loader.dataset[i][0].clamp(0,1).permute(0,2,1).contiguous().permute(2,1,0), cmap=plt.cm.binary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vt8tSeQcy_jZ"
      },
      "source": [
        "We can now create our models, we need a Generator and a Discriminator:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pou-TRizzZXP"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# a few parameters:\n",
        "n_channels = 3\n",
        "img_width = 32\n",
        "\n",
        "# define the generator\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, latent_size=100):\n",
        "        super(Generator, self).__init__()\n",
        "        self.init_size = img_width // 4\n",
        "        self.l1 = nn.Sequential(nn.Linear(latent_size, 128 * self.init_size * self.init_size))\n",
        "\n",
        "        self.conv_blocks = nn.Sequential(\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.Upsample(scale_factor=2),\n",
        "            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(128, 0.8),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Upsample(scale_factor=2),\n",
        "            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(64, 0.8),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(64, n_channels, 3, stride=1, padding=1),\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.l1(x)\n",
        "        x = x.view(x.size(0), 128, self.init_size, self.init_size)\n",
        "        x = self.conv_blocks(x)\n",
        "        return x\n",
        "\n",
        "# define the discriminator\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(n_channels, 64, 3, 2, 1),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(64, 128, 3, 2, 1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(128, 256, 3, 2, 1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(256, 512, 3, 2, 1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(512, 1, 3, 2, 1),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        return x.view(-1, 1).squeeze(1)\n",
        "\n",
        "# instantiate the models\n",
        "G = Generator().to(device)\n",
        "D = Discriminator().to(device)\n",
        "\n",
        "# print the number of parameters\n",
        "print(f'Generator has {sum(p.numel() for p in G.parameters())} parameters.')\n",
        "print(f'Discriminator has {sum(p.numel() for p in D.parameters())} parameters.')\n",
        "\n",
        "# initialise the optimisers\n",
        "optimiser_G = torch.optim.Adam(G.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "optimiser_D = torch.optim.Adam(D.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "print('Optimisers have been created!')\n",
        "\n",
        "# define the loss function\n",
        "criterion = nn.BCELoss()\n",
        "epoch = 0\n",
        "print('Loss function is Binary Cross Entropy!')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBDngw8EueCv"
      },
      "source": [
        "Let's start the main training loop now:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ccMuktrKukIj"
      },
      "outputs": [],
      "source": [
        "# training loop\n",
        "while (epoch<20000):\n",
        "\n",
        "    # arrays for metrics\n",
        "    logs = {}\n",
        "    gen_loss_arr = np.zeros(0)\n",
        "    dis_loss_arr = np.zeros(0)\n",
        "\n",
        "    # iterate over the train dateset\n",
        "    for i, batch in enumerate(train_loader):\n",
        "\n",
        "        x, t = batch\n",
        "        x, t = x.to(device), t.to(device)\n",
        "\n",
        "        # train discriminator\n",
        "        g = G(torch.randn(x.size(0), 100).to(device))\n",
        "        l_r = criterion(D(x).mean(), torch.ones(1)[0].to(device)) # real -> 1\n",
        "        l_f = criterion(D(g.detach()).mean(), torch.zeros(1)[0].to(device)) #  fake -> 0\n",
        "        loss_d = (l_r + l_f)/2.0\n",
        "        optimiser_D.zero_grad()\n",
        "        loss_d.backward()\n",
        "        optimiser_D.step()\n",
        "\n",
        "        # train generator\n",
        "        g = G(torch.randn(x.size(0), 100).to(device))\n",
        "        loss_g = criterion(D(g).mean(), torch.ones(1)[0].to(device)) # fake -> 1\n",
        "        optimiser_G.zero_grad()\n",
        "        loss_g.backward()\n",
        "        optimiser_G.step()\n",
        "\n",
        "        gen_loss_arr = np.append(gen_loss_arr, loss_g.item())\n",
        "        dis_loss_arr = np.append(dis_loss_arr, loss_d.item())\n",
        "\n",
        "    # plot some examples\n",
        "    G.eval()\n",
        "    g = G(torch.randn(x.size(0), 100).to(device))\n",
        "    print('loss D: {:.3f}, loss G: {:.3f}'.format(gen_loss_arr.mean(), dis_loss_arr.mean()))\n",
        "    plt.grid(False)\n",
        "    plt.imshow(torchvision.utils.make_grid(g).cpu().data.clamp(0,1).permute(0,2,1).contiguous().permute(2,1,0), cmap=plt.cm.binary)\n",
        "    plt.show()\n",
        "    plt.pause(0.0001)\n",
        "    G.train()\n",
        "\n",
        "    epoch = epoch+1"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.13 64-bit ('3.9.13')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "fff836058c255167b80f2d77a2226e7e00ff1ecf6518b7f3cd25e9b70384f747"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}