{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DrpK6lIWrhi"
      },
      "source": [
        "# A Simple AutoEncoder in PyTorch\n",
        "---\n",
        "\n",
        "## Author : Amir Atapour-Abarghouei, amir.atapour-abarghouei@durham.ac.uk\n",
        "\n",
        "This notebook will provide an example that shows the implementation of a simple AutoEncoder in PyTorch.\n",
        "\n",
        "Copyright (c) 2024 Amir Atapour-Abarghouei, UK.\n",
        "\n",
        "License : GNU General Public License Version 3\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-exrFDujXlUG"
      },
      "source": [
        "We are going to implement an autoencoder. Let's start by importing what we need:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7Xgr3ZNXmQx"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(f\"Device is {device}!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jRGkDJEkLqt"
      },
      "source": [
        "Let's set a few variables to make things easier:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IGWVNAT5kQqy"
      },
      "outputs": [],
      "source": [
        "batch_size  = 256\n",
        "n_channels  = 3\n",
        "latent_size = 512\n",
        "print('done!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZr-PApBcN4Y"
      },
      "source": [
        "We should now line up the dataset we are going to use. We will be working with our own dataset:\n",
        "\n",
        "**[Radiant X-Ray: A Small Dataset of Processed Chest X-Ray Images](https://github.com/atapour/DGNN-Demos/tree/main/R-X-Ray)**\n",
        "\n",
        "First, let's download the dataset, which is publicly available on GitHub:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q -O R-X-Ray.zip https://github.com/atapour/DGNN-Demos/blob/main/R-X-Ray/R-X-Ray.zip?raw=true\n",
        "!unzip -q R-X-Ray.zip\n",
        "!rm R-X-Ray.zip\n",
        "print('done!')"
      ],
      "metadata": {
        "id": "bVMF4aKY_K9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6AZQ9Kq_1hj"
      },
      "source": [
        "Now we are ready to process our data. We are going to convert our data to 32x32 images to make the work easier and more efficient just for demonstration purposes.\n",
        "\n",
        "Since our dataset does not have classes and is only there to train generative models, we are also going to add a fake class directory to make the implementation easier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wbTxY01eY0Aq"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import torchvision\n",
        "import torch\n",
        "\n",
        "# path to the root directory containing images\n",
        "root_dir = 'R-X-Ray'\n",
        "\n",
        "# check if there are already directories in root_dir\n",
        "subdirectories = [d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))]\n",
        "\n",
        "# if no class directories are found, create one and move the files\n",
        "if not subdirectories:\n",
        "    class_dir = os.path.join(root_dir, 'fakeclass')\n",
        "    os.makedirs(class_dir, exist_ok=True)\n",
        "\n",
        "    # move all images from the root directory to the new subdirectory\n",
        "    for file_name in os.listdir(root_dir):\n",
        "        file_path = os.path.join(root_dir, file_name)\n",
        "        if os.path.isfile(file_path):\n",
        "            shutil.move(file_path, os.path.join(class_dir, file_name))\n",
        "\n",
        "    print(f\"Created directory {class_dir} and moved images into it.\")\n",
        "else:\n",
        "    print(f\"Subdirectories already found: {subdirectories}\")\n",
        "\n",
        "# define the transformations\n",
        "transform = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.Resize([32, 32]),\n",
        "    torchvision.transforms.RandomHorizontalFlip(),\n",
        "    torchvision.transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# create the dataset and dataloader using ImageFolder\n",
        "dataset = torchvision.datasets.ImageFolder(root_dir, transform=transform)\n",
        "\n",
        "# load the dataset\n",
        "train_loader = torch.utils.data.DataLoader(dataset, batch_size=8, shuffle=True, num_workers=2)\n",
        "\n",
        "print(f\"There are {len(dataset)} images in the training set!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5UnQLwJZW6O"
      },
      "source": [
        "Now that the dataset is ready, let's look at a few of our images:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVqAtMBuw8LW"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "for i in range(25):\n",
        "    plt.subplot(5, 5, i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(train_loader.dataset[i][0].clamp(0,1).permute(0,2,1).contiguous().permute(2,1,0), cmap=plt.cm.binary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vt8tSeQcy_jZ"
      },
      "source": [
        "We can now create our model, which will be a very simple convolutional network:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pou-TRizzZXP"
      },
      "outputs": [],
      "source": [
        "# simple block of convolution, batchnorm, and leakyrelu\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, in_f, out_f):\n",
        "        super(Block, self).__init__()\n",
        "        self.f = nn.Sequential(\n",
        "            nn.Conv2d(in_f, out_f, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(out_f),\n",
        "            nn.LeakyReLU(inplace=True)\n",
        "        )\n",
        "    def forward(self,x):\n",
        "        return self.f(x)\n",
        "\n",
        "# define the model\n",
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self, f=16):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encode = nn.Sequential(\n",
        "            Block(n_channels, f),\n",
        "            nn.MaxPool2d(kernel_size=(2,2)), # output = 16x16\n",
        "            Block(f  ,f*2),\n",
        "            nn.MaxPool2d(kernel_size=(2,2)), # output = 8x8\n",
        "            Block(f*2,f*4),\n",
        "            nn.MaxPool2d(kernel_size=(2,2)), # output = 4x4\n",
        "            Block(f*4,f*4),\n",
        "            nn.MaxPool2d(kernel_size=(2,2)), # output = 2x2\n",
        "            Block(f*4,f*4),\n",
        "            nn.MaxPool2d(kernel_size=(2,2)), # output = 1x1\n",
        "            Block(f*4,latent_size),\n",
        "        )\n",
        "\n",
        "        self.decode = nn.Sequential(\n",
        "            nn.Upsample(scale_factor=2), # output = 2x2\n",
        "            Block(latent_size,f*4),\n",
        "            nn.Upsample(scale_factor=2), # output = 4x4\n",
        "            Block(f*4,f*4),\n",
        "            nn.Upsample(scale_factor=2), # output = 8x8\n",
        "            Block(f*4,f*2),\n",
        "            nn.Upsample(scale_factor=2), # output = 16x16\n",
        "            Block(f*2,f  ),\n",
        "            nn.Upsample(scale_factor=2), # output = 32x32\n",
        "            nn.Conv2d(f,n_channels, 3,1,1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "A = Autoencoder().to(device)\n",
        "print(f'The model has {len(torch.nn.utils.parameters_to_vector(A.parameters()))} parameters.')\n",
        "\n",
        "print('The optimiser has been created!')\n",
        "optimiser = torch.optim.Adam(A.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBDngw8EueCv"
      },
      "source": [
        "Let's start the main training loop now:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ccMuktrKukIj"
      },
      "outputs": [],
      "source": [
        "epoch = 0\n",
        "# training loop\n",
        "while (epoch < 20):\n",
        "\n",
        "    # for metrics\n",
        "    loss_arr = np.zeros(0)\n",
        "\n",
        "    # iterate over the training dateset\n",
        "    for i, batch in enumerate(train_loader):\n",
        "\n",
        "        # sample x from the dataset\n",
        "        x, t = batch\n",
        "        x, t = x.to(device), t.to(device)\n",
        "\n",
        "        # do the forward pass with mean squared error\n",
        "        z = A.encode(x)\n",
        "        x_hat = A.decode(z)\n",
        "        # calculate loss:\n",
        "        loss = ((x-x_hat)**2).mean()\n",
        "\n",
        "        # backpropagate to compute the gradients of the loss w.r.t the parameters and optimise\n",
        "        optimiser.zero_grad()\n",
        "        loss.backward()\n",
        "        optimiser.step()\n",
        "\n",
        "        # collect stats\n",
        "        loss_arr = np.append(loss_arr, loss.item())\n",
        "\n",
        "    # sample\n",
        "    z = torch.randn_like(z)\n",
        "    g = A.decode(z)\n",
        "\n",
        "    # plot some examples from training\n",
        "    print(\"\\n============================================\")\n",
        "    print(f'Epoch {epoch} Loss: {loss.mean().item()}')\n",
        "    print('Training Examples')\n",
        "    plt.grid(False)\n",
        "    plt.imshow(torchvision.utils.make_grid(x_hat[:16]).cpu().data.permute(0,2,1).contiguous().permute(2,1,0), cmap=plt.cm.binary)\n",
        "    plt.show()\n",
        "\n",
        "    # plot results sampled from latent\n",
        "    print('Samples from the Latent Space')\n",
        "    plt.grid(False)\n",
        "    plt.imshow(torchvision.utils.make_grid(g[:16]).cpu().data.permute(0,2,1).contiguous().permute(2,1,0), cmap=plt.cm.binary)\n",
        "    plt.show()\n",
        "    print(\"============================================\\n\")\n",
        "\n",
        "    epoch = epoch+1"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.13 64-bit ('3.9.13')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "fff836058c255167b80f2d77a2226e7e00ff1ecf6518b7f3cd25e9b70384f747"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}